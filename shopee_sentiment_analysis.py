# -*- coding: utf-8 -*-
"""Copy of Shopee_Sentiment_Assignment (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jcSDBxOqeb7irtJ_KCxeTkXdJNGI8UYH

# Shopee Reviews Sentiment Analysis (Positive / Neutral / Negative)

Implementing a full sentiment-analysis:
- Uses a **public review corpus** (Shopee app reviews) and clearly defines the **task** (polarity: positive/neutral/negative).
- Implements ** 2 machine-learning techniques** for sentiment classification.
- Including **pre-processing**, **feature engineering**, **hyperparameter tuning**, and **evaluation**.
- Producing **multiple graphs** for EDA and for evaluation/results.

> Dataset files expected in the same folder:
> - `scrapped_Shopee 12.12_EN_.csv`
> - `scrapped_Shopee 12.12_ID_.csv`

## 1) Load data and basic cleaning
"""

import pandas as pd
import numpy as np

PATH_EN = "scrapped_Shopee 12.12_EN_.csv"
PATH_ID = "scrapped_Shopee 12.12_ID_.csv"

df_en = pd.read_csv(PATH_EN)
df_id = pd.read_csv(PATH_ID)

df_en["lang"] = "EN"
df_id["lang"] = "ID"

df = pd.concat([df_en, df_id], ignore_index=True)

print("Shape:", df.shape)
print(df.head())
print(df.columns)

"""## 2) Filter to recent 5 years and create sentiment labels (pos/neu/neg)

Creating polarity labels from the review star score:

- **1–2 → Negative**
- **3 → Neutral**
- **4–5 → Positive**

⚠️ To avoid label leakage, we **do not use `score` as an input feature** after creating the label.

"""

import pandas as pd

# Parse datetime
df["at"] = pd.to_datetime(df["at"], errors="coerce")

# Keep only recent 5 years (relative to latest date in your dataset)
latest = df["at"].max()
cutoff = latest - pd.DateOffset(years=5)
df = df[df["at"] >= cutoff].copy()

# Keep rows with non-empty text and valid score
df["content"] = df["content"].astype(str)
df = df[df["content"].str.strip().str.len() > 0]
df = df[df["score"].between(1, 5)]

def score_to_sentiment(score: int) -> str:
    if score <= 2:
        return "negative"
    elif score == 3:
        return "neutral"
    else:
        return "positive"

df["sentiment"] = df["score"].apply(score_to_sentiment)

print("Filtered shape:", df.shape)
print("Date range:", df["at"].min(), "→", df["at"].max())
df["sentiment"].value_counts()

"""## 3) Text pre-processing + feature engineering

Applying lightweight text cleaning (URLs, extra spaces).  
Then Engineering extra features from text + metadata to strengthen performance.

"""

import re

URL_RE = re.compile(r"http\S+|www\.\S+")
WS_RE = re.compile(r"\s+")
# Basic emoji range (not perfect, but works well for counting)
EMOJI_RE = re.compile(r"[\U00010000-\U0010ffff]", flags=re.UNICODE)

def clean_text(t: str) -> str:
    t = t.lower()
    t = URL_RE.sub(" ", t)
    t = t.replace("\n", " ").replace("\r", " ")
    t = WS_RE.sub(" ", t).strip()
    return t

df["clean_content"] = df["content"].map(clean_text)

# Numeric / boolean engineered features
df["text_len"] = df["clean_content"].str.len()
df["word_count"] = df["clean_content"].str.split().map(len)
df["exclamation_count"] = df["content"].astype(str).str.count("!")
df["question_count"] = df["content"].astype(str).str.count(r"\?")
df["emoji_count"] = df["content"].astype(str).map(lambda x: len(EMOJI_RE.findall(x)))
df["has_reply"] = df["replyContent"].notna().astype(int)

# Time-based features
df["review_hour"] = df["at"].dt.hour.fillna(0).astype(int)
df["review_dow"] = df["at"].dt.dayofweek.fillna(0).astype(int)

df[["clean_content","text_len","word_count","emoji_count","has_reply","lang","sentiment"]].head()

"""## 4) Exploratory Data Analysis (EDA) graphs

Exploratory visualization, Graphs for critical analysis.

"""

import matplotlib.pyplot as plt

# 1) Sentiment distribution
sent_counts = df["sentiment"].value_counts()

plt.figure(figsize=(6,4))
plt.bar(sent_counts.index, sent_counts.values)
plt.title("Sentiment Distribution (Polarity Labels)")
plt.xlabel("Sentiment")
plt.ylabel("Number of reviews")
plt.show()

# 2) Rating distribution (original score)
score_counts = df["score"].value_counts().sort_index()
plt.figure(figsize=(6,4))
plt.bar(score_counts.index.astype(str), score_counts.values)
plt.title("Star Rating Distribution (1–5)")
plt.xlabel("Score")
plt.ylabel("Number of reviews")
plt.show()

# 3) Thumbs-up counts by sentiment (boxplot-like using matplotlib)
# (Many outliers: we clip for readability)
clip_max = np.percentile(df["thumbsUpCount"], 99)
data = [
    df.loc[df["sentiment"]=="negative","thumbsUpCount"].clip(0, clip_max),
    df.loc[df["sentiment"]=="neutral","thumbsUpCount"].clip(0, clip_max),
    df.loc[df["sentiment"]=="positive","thumbsUpCount"].clip(0, clip_max),
]
plt.figure(figsize=(7,4))
plt.boxplot(data, labels=["negative","neutral","positive"], showfliers=False)
plt.title("Thumbs-up Count by Sentiment (clipped at 99th percentile)")
plt.ylabel("thumbsUpCount")
plt.show()

"""## 5) Build ML pipelines (TWO models) + hyperparameter tuning

Using:
1. **Logistic Regression** (multinomial)  
2. **Linear SVM** (LinearSVC)

Both are strong baselines for text classification.

Combining:
- TF-IDF vectorization on cleaned text
- Numeric engineered features (scaled)
- One-hot encoding for language (EN/ID)

"""

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

import numpy as np
import time
from sklearn.base import clone
from sklearn.metrics import f1_score

# Features
TEXT_COL = "clean_content"
NUM_COLS = ["thumbsUpCount","text_len","word_count","exclamation_count","question_count",
            "emoji_count","has_reply","review_hour","review_dow"]
CAT_COLS = ["lang"]
TARGET = "sentiment"

X = df[[TEXT_COL] + NUM_COLS + CAT_COLS].copy()
y = df[TARGET].copy()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Fast OneHotEncoder (works across sklearn versions)
try:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True, dtype=np.float32)
except TypeError:
    ohe = OneHotEncoder(handle_unknown="ignore", sparse=True, dtype=np.float32)

preprocess_fast = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(
            ngram_range=(1, 1),       # FAST: unigrams only
            min_df=8,                 # FAST: remove rare terms
            max_df=0.9,
            sublinear_tf=True,
            max_features=20000,       # FAST: cap vocab
            dtype=np.float32
        ), TEXT_COL),
        ("num", MaxAbsScaler(), NUM_COLS),
        ("cat", ohe, CAT_COLS),
    ],
    remainder="drop"
)

# Split for fast tuning
X_subtrain, X_val, y_subtrain, y_val = train_test_split(
    X_train, y_train, test_size=0.10, random_state=42, stratify=y_train
)

# Tune on a fraction (fast)
TUNE_FRAC = 0.20  # reduce to 0.10 if still slow
X_tune, _, y_tune, _ = train_test_split(
    X_subtrain, y_subtrain, train_size=TUNE_FRAC, random_state=42, stratify=y_subtrain
)

# Fit preprocess ONCE
prep = clone(preprocess_fast)
t0 = time.time()
X_tune_mat = prep.fit_transform(X_tune)
X_val_mat = prep.transform(X_val)
print(f"Preprocess done in {time.time() - t0:.1f}s")

def tune_C(estimator, C_list, Xtr, ytr, Xva, yva):
    bestC, bestF = None, -1
    for C in C_list:
        est = clone(estimator).set_params(C=C)
        t0 = time.time()
        est.fit(Xtr, ytr)
        pred = est.predict(Xva)
        f = f1_score(yva, pred, average="macro")
        print(f"  C={C:<4} macro-F1={f:.4f}  (fit {time.time()-t0:.1f}s)")
        if f > bestF:
            bestC, bestF = C, f
    return bestC, bestF

# Base models for tuning (fast settings)
lr_base = LogisticRegression(
    solver="sag",
    class_weight="balanced",
    max_iter=700,
    tol=1e-3
)

svm_base = LinearSVC(
    class_weight="balanced",
    tol=1e-3,
    max_iter=2000
)

C_list = [0.5, 1.0, 2.0]

print("\nTuning LR...")
bestC_lr, bestF_lr = tune_C(lr_base, C_list, X_tune_mat, y_tune, X_val_mat, y_val)

print("\nTuning SVM...")
bestC_svm, bestF_svm = tune_C(svm_base, C_list, X_tune_mat, y_tune, X_val_mat, y_val)

print("\nChosen C values:")
print("  LR :", bestC_lr, "val macro-F1:", bestF_lr)
print("  SVM:", bestC_svm, "val macro-F1:", bestF_svm)

# Fit preprocess on FULL training set once
prep_final = clone(preprocess_fast)

t0 = time.time()
X_train_mat = prep_final.fit_transform(X_train)
X_test_mat = prep_final.transform(X_test)
print(f"Final preprocess done in {time.time() - t0:.1f}s")

# Final models (stronger fit)
lr_final = LogisticRegression(
    C=bestC_lr,
    solver="sag",
    class_weight="balanced",
    max_iter=1400,
    tol=1e-4
)

svm_final = LinearSVC(
    C=bestC_svm,
    class_weight="balanced",
    tol=1e-3,
    max_iter=4000
)

t0 = time.time()
lr_final.fit(X_train_mat, y_train)
print(f"Final LR trained in {time.time()-t0:.1f}s")

t0 = time.time()
svm_final.fit(X_train_mat, y_train)
print(f"Final SVM trained in {time.time()-t0:.1f}s")

# You will use these for evaluation later:
# prep_final, lr_final, svm_final, X_test_mat

"""## 6) Evaluate both models + required graphs (confusion matrix....)


- classification report (precision/recall/F1)
- confusion matrix
- comparison chart between models

"""

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score

models = {
    "LogReg": lr_final,
    "LinearSVM": svm_final
}

results = []

for name, model in models.items():
    # Note: For prediction, the input X_test must be preprocessed using the final preprocessor
    # The models (lr_final, svm_final) were trained on X_train_mat, which is the preprocessed X_train.
    # So, prediction should be done on X_test_mat.
    y_pred = model.predict(X_test_mat)
    acc = accuracy_score(y_test, y_pred)
    f1m = f1_score(y_test, y_pred, average="macro")
    results.append((name, acc, f1m))
    print("\n" + "="*60)
    print(name)
    print("Accuracy:", acc)
    print("Macro-F1:", f1m)
    print(classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(values_format="d")
    plt.title(f"Confusion Matrix - {name}")
    plt.show()

# Model comparison bar chart
names = [r[0] for r in results]
accs = [r[1] for r in results]
f1ms = [r[2] for r in results]

x = np.arange(len(names))
plt.figure(figsize=(7,4))
plt.bar(x - 0.2, accs, width=0.4, label="Accuracy")
plt.bar(x + 0.2, f1ms, width=0.4, label="Macro-F1")
plt.xticks(x, names)
plt.ylim(0, 1.0)
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.legend()
plt.show()

"""## 7) Sentiment trend over time (result visualization)

Helping to **interpret sentiment patterns** (e.g., spikes in negative sentiment).

"""

# Daily sentiment proportions
tmp = df.copy()
tmp["date"] = tmp["at"].dt.date

daily = (
    tmp.groupby(["date","sentiment"])
       .size()
       .reset_index(name="count")
)

pivot = daily.pivot(index="date", columns="sentiment", values="count").fillna(0)
pivot = pivot.sort_index()

# Convert to proportions
prop = pivot.div(pivot.sum(axis=1), axis=0)

plt.figure(figsize=(10,4))
for col in ["negative","neutral","positive"]:
    if col in prop.columns:
        plt.plot(prop.index, prop[col], label=col)
plt.title("Daily Sentiment Proportions (Shopee Reviews)")
plt.xlabel("Date")
plt.ylabel("Proportion")
plt.legend()
plt.tight_layout()
plt.show()

"""## 8) Simple demo: predict sentiment for new text

Using the best model (by Macro-F1) and test a few inputs.

"""

best_name, best_acc, best_f1 = sorted(results, key=lambda x: x[2], reverse=True)[0]
best_model = models[best_name]
print("Best model:", best_name, "Macro-F1:", best_f1)

def predict_sentiment(text: str, thumbsUpCount: int = 0, lang: str = "EN"):
    row = {
        "clean_content": clean_text(text),
        "thumbsUpCount": thumbsUpCount,
        "text_len": len(clean_text(text)),
        "word_count": len(clean_text(text).split()),
        "exclamation_count": text.count("!"),
        "question_count": text.count("?"),
        "emoji_count": len(EMOJI_RE.findall(text)),
        "has_reply": 0,
        "review_hour": 0,
        "review_dow": 0,
        "lang": lang
    }
    X_new = pd.DataFrame([row])
    # Apply the same preprocessor used for training to the new input
    X_new_transformed = prep_final.transform(X_new)
    return best_model.predict(X_new_transformed)[0]

examples = [
    ("Shopee is super fast and the vouchers are great!", 5, "EN"),
    ("App crash terus, susah nak login. Very disappointed.", 0, "EN"),
    ("ok lah biasa je", 0, "ID"),
]
for t, like, lg in examples:
    print(lg, "→", t, "=>", predict_sentiment(t, thumbsUpCount=like, lang=lg))

"""## 9) Save the best trained model (for your demo/application)

Creating a file that can be reused later (e.g., in a Streamlit app).

"""

import os
import sklearn
import joblib

print("sklearn:", sklearn.__version__)

# best_model is lr_final or svm_final (classifier), so save it together with the preprocessor
bundle = {"prep": prep_final, "model": best_model}
joblib.dump(bundle, "best_shopee_sentiment_model.joblib")

print("Saved:", os.path.abspath("best_shopee_sentiment_model.joblib"))

